---
title: "Project"
output: html_document
date: "`r Sys.Date()`"
---

# Set ups
```{r}
# Load Data
library(tidyverse)
library(forcats)
library(smotefamily)

# Set seeds
set.seed(521)
```


# EDA
```{r}
# Load data
nepal <- readr::read_csv("nepal_dat.csv")
dim(nepal); glimpse(nepal)
```

```{r}
# check for NAs
colSums(is.na(nepal))
```

```{r}
data.frame(
  class = sapply(nepal, class)
)

```

```{r}
nepal_chr <- nepal |> mutate(across(everything(), as.character))
str(nepal_chr, strict.width = "cut", width = 80)

```

```{r}
numeric_cols <- c(
  "age",
  "area_percentage",
  "height_percentage",
  "count_floors_pre_eq",
  "count_families", 
  "geo_level_1_id", 
  "geo_level_2_id", 
  "geo_level_3_id"
)
numeric_cols <- intersect(numeric_cols, names(nepal_chr))  # keep only those present
numeric_cols

```


```{r}
target_col <- "damage_grade"
all_cols <- names(nepal_chr)
categorical_cols <- setdiff(all_cols, c(numeric_cols, target_col))
categorical_cols

```


```{r}
nepal <- nepal_chr |>
  mutate(
    across(all_of(numeric_cols), ~ readr::parse_double(.x)),   # <- no () mistake
    !!target_col := factor(.data[[target_col]],
                           levels = c("1","2","3"),
                           ordered = TRUE),
    across(all_of(categorical_cols), ~ factor(.x)),
    across(all_of(categorical_cols), ~ forcats::fct_lump_min(.x, min = 500))
  )

str(nepal, strict.width = "cut", width = 80)

```


```{r}
total_NA <- sum(colSums(is.na(nepal)))
total_NA

print(
  nepal |>
    dplyr::select(all_of(numeric_cols)) |>
    summary()
)


lvl_counts <- sapply(nepal[categorical_cols], nlevels)
head(sort(lvl_counts, decreasing = TRUE))

```


```{r}
plot_df <- nepal 
if (length(numeric_cols) > 0) {
  plot_df |>
    dplyr::select(all_of(numeric_cols)) |>
    tidyr::pivot_longer(everything(), names_to = "variable", values_to = "value") |>
    ggplot(aes(x = variable, y = value)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free", ncol = 4) +
    theme_minimal(base_size = 12) +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
    labs(title = "Boxplots of Numeric Variables", x = NULL, y = "Value")
}


```

```{r}
if (length(numeric_cols) > 0) {
  plot_df |>
    dplyr::select(all_of(numeric_cols)) |>
    tidyr::pivot_longer(everything(), names_to = "variable", values_to = "value") |>
    ggplot(aes(x = value)) +
    geom_histogram(bins = 40) +
    facet_wrap(~ variable, scales = "free", ncol = 4) +
    theme_minimal(base_size = 12) +
    labs(title = "Histograms of Numeric Variables", x = "Value", y = "Count")
}

```

```{r}
cat_cols_30 <- categorical_cols
if (length(cat_cols_30) > 30) cat_cols_30 <- cat_cols_30[1:30]

nepal |>
  dplyr::select(all_of(cat_cols_30[1:10])) |>
  dplyr::mutate(dplyr::across(dplyr::everything(), as.character)) |>
  tidyr::pivot_longer(dplyr::everything(),
                      names_to = "variable", values_to = "category") |>
  ggplot(aes(x = category)) +
  geom_bar(fill = "tomato") +
  facet_wrap(~ variable, scales = "free_x", ncol = 3) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 7),
        plot.title  = element_text(face = "bold")) +
  labs(title = "Distribution of Categorical Variables (1–10)",
       x = "Category", y = "Count")


```


```{r}
nepal |>
  dplyr::select(all_of(cat_cols_30[11:20])) |>
  dplyr::mutate(dplyr::across(dplyr::everything(), as.character)) |>
  tidyr::pivot_longer(dplyr::everything(),
                      names_to = "variable", values_to = "category") |>
  ggplot(aes(x = category)) +
  geom_bar(fill = "tomato") +
  facet_wrap(~ variable, scales = "free_x", ncol = 3) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 7),
        plot.title  = element_text(face = "bold")) +
  labs(title = "Distribution of Categorical Variables (11–20)",
       x = "Category", y = "Count")

```

```{r}
nepal |>
  dplyr::select(all_of(cat_cols_30[21:30])) |>
  dplyr::mutate(dplyr::across(dplyr::everything(), as.character)) |>
  tidyr::pivot_longer(dplyr::everything(),
                      names_to = "variable", values_to = "category") |>
  ggplot(aes(x = category)) +
  geom_bar(fill = "tomato") +
  facet_wrap(~ variable, scales = "free_x", ncol = 3) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 7),
        plot.title  = element_text(face = "bold")) +
  labs(title = "Distribution of Categorical Variables (21–30)",
       x = "Category", y = "Count")

```

# Feature engineering
```{r}
# Use SMOTE to make data more balanced
p_min <- 0.25

y <- nepal[["damage_grade"]]
X <- nepal[, setdiff(names(nepal), "damage_grade"), drop = FALSE]

# LABEL ENCODING for categorical columns
label_encode <- function(col) {
  if (is.factor(col) | is.character(col)) {
    return(as.numeric(factor(col))) 
  } else {
    return(col)
  }
}

X_le <- as.data.frame(lapply(X, label_encode))

tab <- table(y)
levs <- names(tab)
maj_n <- max(tab)
floor_n <- ceiling(p_min * maj_n)

n_c <- as.numeric(tab)
dup_size <- ceiling(pmax(0, floor_n - n_c) / pmax(1, n_c))
names(dup_size) <- levs

tab_before <- table(y)
prop_before <- prop.table(tab_before)
cat("Class proportions before SMOTE:\n")
print(round(prop_before, 4))

sm_out <- SMOTE(
  X = as.data.frame(X_le), target = y, K = 5,
  dup_size = dup_size
)

nepal_smote <- as.data.frame(sm_out$data)
colnames(nepal_smote)[ncol(nepal_smote)] <- "damage_grade"

tblA <- table(nepal_smote[["damage_grade"]])
maj_lvl <- names(which.max(tblA))
maj_nA <- as.integer(max(tblA))
others <- tblA[names(tblA) != maj_lvl]
sum_others <- sum(others)
m_caps <- floor((as.numeric(others) / p_min) - sum_others)
m_target <- min(c(m_caps, maj_nA))

idx_maj <- which(nepal_smote[["damage_grade"]] == maj_lvl)
keep_maj <- sample(idx_maj, size = m_target)
keep_all <- c(keep_maj, which(nepal_smote[["damage_grade"]] != maj_lvl))
nepal_smote <- nepal_smote[keep_all, , drop = FALSE]

cat("Class proportions after balancing:\n")
print(round(prop.table(table(nepal_smote[["damage_grade"]])), 4))
cat(sprintf("\nFinal shape : %d rows × %d cols\n", nrow(nepal_smote), ncol(nepal_smote)))
```

```{r}
# standard scaling nuermical columns
nepal_smote_scaled <- nepal_smote
nepal_smote_scaled[numeric_cols] <- scale(nepal_smote[numeric_cols])

summary(nepal_smote_scaled[numeric_cols])
```

```{r}
plot_df_2 <- nepal_smote_scaled
if (length(numeric_cols) > 0) {
  plot_df_2 |>
    dplyr::select(all_of(numeric_cols)) |>
    tidyr::pivot_longer(everything(), names_to = "variable", values_to = "value") |>
    ggplot(aes(x = variable, y = value)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free", ncol = 4) +
    theme_minimal(base_size = 12) +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
    labs(title = "Boxplots of Numeric Variables after SMOTE and Standard Scaling", x = NULL, y = "Value")
}
```

```{r}
if (length(numeric_cols) > 0) {
  plot_df_2 |>
    dplyr::select(all_of(numeric_cols)) |>
    tidyr::pivot_longer(everything(), names_to = "variable", values_to = "value") |>
    ggplot(aes(x = value)) +
    geom_histogram(bins = 40) +
    facet_wrap(~ variable, scales = "free", ncol = 4) +
    theme_minimal(base_size = 12) +
    labs(title = "Histograms of Numeric Variables After SMOTE and Standard Scaling", x = "Value", y = "Count")
}
```


# GLM with multinomial link
```{r}
library(nnet)

glm_dat <- nepal_smote_scaled

multi_fit_all <- multinom(
  damage_grade ~ .,
  data  = glm_dat,
  trace = TRUE
)

summary(multi_fit_all)

```

```{r}
pred_class <- predict(multi_fit_all, newdata = glm_dat)              # factor
pred_prob  <- predict(multi_fit_all, newdata = glm_dat, type = "probs")
```

```{r}
# Make sure both are factors
true_y  <- factor(glm_dat$damage_grade)
pred_y  <- factor(pred_class)

# Force matching levels
common_levels <- levels(true_y)
pred_y <- factor(pred_y, levels = common_levels)

# Now compute confusion matrix
library(caret)
cm <- confusionMatrix(pred_y, true_y)
cm

kappa
```


This barely predicts from simply predicting class2 and is performing badly, so we consider some alternative approaches to the scaled data.

# Train/Test Split (for classifications)
```{r}
idx_train <- createDataPartition(nepal_smote_scaled$damage_grade, p = 0.8, list = FALSE)
train <- nepal_smote_scaled[idx_train, ]
test  <- nepal_smote_scaled[-idx_train, ]
```

# RF
```{r}
library(randomForest)

# If damage_grade is ordered, drop ordering for RF:
train_rf <- train
train_rf$damage_grade <- factor(train_rf$damage_grade)
test_rf  <- test
test_rf$damage_grade  <- factor(test_rf$damage_grade)

rf_mod <- randomForest(
  damage_grade ~ .,
  data = train_rf,
  ntree = 10,
  mtry  = floor(sqrt(ncol(train_rf) - 1)),  # heuristic
  importance = TRUE
)

pred_rf <- predict(rf_mod, newdata = test_rf)

cm_rf <- confusionMatrix(pred_rf, test_rf$damage_grade)
cm_rf 
```

```{r}
library(ranger)

set.seed(521)

train_rf <- train
train_rf$damage_grade <- factor(train_rf$damage_grade)
test_rf  <- test
test_rf$damage_grade  <- factor(test_rf$damage_grade)

p <- ncol(train_rf) - 1  # number of predictors

rf_mod_fast <- ranger(
  formula = damage_grade ~ .,
  data = train_rf,
  num.trees = 600,                     # fewer trees than 500
  mtry = floor(sqrt(p)),              # standard heuristic
  min.node.size = 20,                 # slightly larger leaf size → faster
  importance = "impurity",
  respect.unordered.factors = "order" # handles factors efficiently
)

# Predictions
pred_rf <- predict(rf_mod_fast, data = test_rf)$predictions
pred_rf <- factor(pred_rf, levels = levels(test_rf$damage_grade))

cm_rf <- caret::confusionMatrix(pred_rf, test_rf$damage_grade)
cm_rf
```

```{r}
# -------------------------------
# 1. Use SMOTE + scaled dataset
# -------------------------------
dat <- nepal_smote_scaled

# Ensure damage_grade is a proper factor with levels "1","2","3"
dat$damage_grade <- factor(dat$damage_grade, levels = c("1", "2", "3"))

# -------------------------------
# 2. Train/test split (stratified)
# -------------------------------
idx_train <- createDataPartition(dat$damage_grade, p = 0.8, list = FALSE)
train <- dat[idx_train, ]
test  <- dat[-idx_train, ]

# Confirm factor levels are consistent
train$damage_grade <- droplevels(train$damage_grade)
test$damage_grade  <- factor(test$damage_grade,
                             levels = levels(train$damage_grade))

# -------------------------------
# 3. Prepare data for CatBoost
# -------------------------------
# Separate predictors and target
X_train <- subset(train, select = -damage_grade)
X_test  <- subset(test,  select = -damage_grade)

# Numeric class labels for CatBoost: 0,1,2
y_train_num <- as.integer(train$damage_grade) - 1
y_test_num  <- as.integer(test$damage_grade)  - 1

# Create CatBoost pools (all predictors numeric at this point)
train_pool <- catboost.load_pool(
  data  = as.matrix(X_train),
  label = y_train_num
)
test_pool <- catboost.load_pool(
  data  = as.matrix(X_test),
  label = y_test_num
)

# -------------------------------
# 4. Train CatBoost model
# -------------------------------
cat_params <- list(
  loss_function = "MultiClass",
  eval_metric   = "MultiClass",
  iterations    = 3000,
  learning_rate = 0.08,
  depth         = 8,
  l2_leaf_reg   = 5,
  bagging_temperature = 1,
  rsm          = 0.9,
  random_seed  = 521,
  od_type      = "Iter",
  od_wait      = 80
)


cat_model <- catboost.train(
  learn_pool = train_pool,
  test_pool  = NULL,
  params     = cat_params
)

# -------------------------------
# 5. Predictions + confusion matrix
# -------------------------------
# Get class probabilities: n x 3 matrix
pred_prob_cat <- catboost.predict(
  model = cat_model,
  pool  = test_pool,
  prediction_type = "Probability"
)

# Pick class with highest probability (0,1,2)
pred_idx0 <- max.col(pred_prob_cat) - 1

# Map 0,1,2 back to "1","2","3"
dg_levels <- levels(train$damage_grade)  # c("1","2","3")
pred_cat <- factor(dg_levels[pred_idx0 + 1], levels = dg_levels)

# Confusion matrix
cm_cat <- confusionMatrix(pred_cat, test$damage_grade)
cm_cat

# Optional: extract accuracy + balanced accuracy
cat_metrics <- data.frame(
  Accuracy         = cm_cat$overall["Accuracy"],
  BalancedAccuracy = mean(cm_cat$byClass[, "Balanced Accuracy"])
)
cat_metrics
```
