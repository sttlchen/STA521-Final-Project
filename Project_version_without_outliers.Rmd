---
title: "Project"
output: html_document
date: "`r Sys.Date()`"
---

# Set ups
```{r}
# Load Data
library(tidyverse)
library(forcats)
library(smotefamily)

# Set seeds
set.seed(521)
```


# EDA
```{r}
# Load data
nepal <- readr::read_csv("nepal_dat.csv")
nepal_test <- readr::read_csv("nepal_test.csv")
dim(nepal); glimpse(nepal)
```

```{r}
# check for NAs
colSums(is.na(nepal))
```

```{r}
data.frame(
  class = sapply(nepal, class)
)

```

```{r}
nepal_chr <- nepal |> mutate(across(everything(), as.character))
str(nepal_chr, strict.width = "cut", width = 80)

```

```{r}
numeric_cols <- c(
  "age",
  "area_percentage",
  "height_percentage",
  "count_floors_pre_eq",
  "count_families", 
  "geo_level_1_id", 
  "geo_level_2_id", 
  "geo_level_3_id"
)
numeric_cols <- intersect(numeric_cols, names(nepal_chr))  # keep only those present
numeric_cols

```


```{r}
target_col <- "damage_grade"
all_cols <- names(nepal_chr)
categorical_cols <- setdiff(all_cols, c(numeric_cols, target_col))
categorical_cols

```


```{r}
nepal <- nepal_chr |>
  mutate(
    across(all_of(numeric_cols), ~ readr::parse_double(.x)),   # <- no () mistake
    !!target_col := factor(.data[[target_col]],
                           levels = c("1","2","3"),
                           ordered = TRUE),
    across(all_of(categorical_cols), ~ factor(.x)),
    across(all_of(categorical_cols), ~ forcats::fct_lump_min(.x, min = 500))
  )

str(nepal, strict.width = "cut", width = 80)

```

```{r}
# Outlier detection with Isolation Forest
library(isotree)

x_if <- nepal |>
  dplyr::select(all_of(numeric_cols)) |>
  as.matrix()

set.seed(521)

# Fit Isolation Forest
iso_mod <- isolation.forest(
  x_if,
  ntrees      = 200,   # number of trees
  sample_size = 256    # subsample size per tree
)

# Anomaly scores: larger = more "outlier-like"
scores <- predict(iso_mod, x_if, type = "score")

summary(scores)
quantile(scores, probs = c(0.95, 0.97, 0.99))

# Choose a cutoff: top 2% most anomalous as outliers
cutoff     <- quantile(scores, 0.98)
is_outlier <- scores > cutoff

table(is_outlier)
table(nepal$damage_grade[is_outlier])

# Keep only non-outliers
nepal <- nepal[!is_outlier, , drop = FALSE]

dim(nepal)
```

```{r}
total_NA <- sum(colSums(is.na(nepal)))
total_NA

print(
  nepal |>
    dplyr::select(all_of(numeric_cols)) |>
    summary()
)


lvl_counts <- sapply(nepal[categorical_cols], nlevels)
head(sort(lvl_counts, decreasing = TRUE))

```


```{r}
plot_df <- nepal 
if (length(numeric_cols) > 0) {
  plot_df |>
    dplyr::select(all_of(numeric_cols)) |>
    tidyr::pivot_longer(everything(), names_to = "variable", values_to = "value") |>
    ggplot(aes(x = variable, y = value)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free", ncol = 4) +
    theme_minimal(base_size = 12) +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
    labs(title = "Boxplots of Numeric Variables", x = NULL, y = "Value")
}


```

```{r}
if (length(numeric_cols) > 0) {
  plot_df |>
    dplyr::select(all_of(numeric_cols)) |>
    tidyr::pivot_longer(everything(), names_to = "variable", values_to = "value") |>
    ggplot(aes(x = value)) +
    geom_histogram(bins = 40) +
    facet_wrap(~ variable, scales = "free", ncol = 4) +
    theme_minimal(base_size = 12) +
    labs(title = "Histograms of Numeric Variables", x = "Value", y = "Count")
}

```

```{r}
cat_cols_30 <- categorical_cols
if (length(cat_cols_30) > 30) cat_cols_30 <- cat_cols_30[1:30]

nepal |>
  dplyr::select(all_of(cat_cols_30[1:10])) |>
  dplyr::mutate(dplyr::across(dplyr::everything(), as.character)) |>
  tidyr::pivot_longer(dplyr::everything(),
                      names_to = "variable", values_to = "category") |>
  ggplot(aes(x = category)) +
  geom_bar(fill = "tomato") +
  facet_wrap(~ variable, scales = "free_x", ncol = 3) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 7),
        plot.title  = element_text(face = "bold")) +
  labs(title = "Distribution of Categorical Variables (1–10)",
       x = "Category", y = "Count")


```


```{r}
nepal |>
  dplyr::select(all_of(cat_cols_30[11:20])) |>
  dplyr::mutate(dplyr::across(dplyr::everything(), as.character)) |>
  tidyr::pivot_longer(dplyr::everything(),
                      names_to = "variable", values_to = "category") |>
  ggplot(aes(x = category)) +
  geom_bar(fill = "tomato") +
  facet_wrap(~ variable, scales = "free_x", ncol = 3) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 7),
        plot.title  = element_text(face = "bold")) +
  labs(title = "Distribution of Categorical Variables (11–20)",
       x = "Category", y = "Count")

```

```{r}
nepal |>
  dplyr::select(all_of(cat_cols_30[21:30])) |>
  dplyr::mutate(dplyr::across(dplyr::everything(), as.character)) |>
  tidyr::pivot_longer(dplyr::everything(),
                      names_to = "variable", values_to = "category") |>
  ggplot(aes(x = category)) +
  geom_bar(fill = "tomato") +
  facet_wrap(~ variable, scales = "free_x", ncol = 3) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 7),
        plot.title  = element_text(face = "bold")) +
  labs(title = "Distribution of Categorical Variables (21–30)",
       x = "Category", y = "Count")

```

# Feature engineering
```{r}
# Use SMOTE to make data more balanced
p_min <- 0.25

y <- nepal[["damage_grade"]]
X <- nepal[, setdiff(names(nepal), "damage_grade"), drop = FALSE]

# LABEL ENCODING for categorical columns
label_encode <- function(col) {
  if (is.factor(col) | is.character(col)) {
    return(as.numeric(factor(col))) 
  } else {
    return(col)
  }
}

X_le <- as.data.frame(lapply(X, label_encode))

tab <- table(y)
levs <- names(tab)
maj_n <- max(tab)
floor_n <- ceiling(p_min * maj_n)

n_c <- as.numeric(tab)
dup_size <- ceiling(pmax(0, floor_n - n_c) / pmax(1, n_c))
names(dup_size) <- levs

tab_before <- table(y)
prop_before <- prop.table(tab_before)
cat("Class proportions before SMOTE:\n")
print(round(prop_before, 4))

sm_out <- SMOTE(
  X = as.data.frame(X_le), target = y, K = 5,
  dup_size = dup_size
)

nepal_smote <- as.data.frame(sm_out$data)
colnames(nepal_smote)[ncol(nepal_smote)] <- "damage_grade"

tblA <- table(nepal_smote[["damage_grade"]])
maj_lvl <- names(which.max(tblA))
maj_nA <- as.integer(max(tblA))
others <- tblA[names(tblA) != maj_lvl]
sum_others <- sum(others)
m_caps <- floor((as.numeric(others) / p_min) - sum_others)
m_target <- min(c(m_caps, maj_nA))

idx_maj <- which(nepal_smote[["damage_grade"]] == maj_lvl)
keep_maj <- sample(idx_maj, size = m_target)
keep_all <- c(keep_maj, which(nepal_smote[["damage_grade"]] != maj_lvl))
nepal_smote <- nepal_smote[keep_all, , drop = FALSE]

cat("Class proportions after balancing:\n")
print(round(prop.table(table(nepal_smote[["damage_grade"]])), 4))
cat(sprintf("\nFinal shape : %d rows × %d cols\n", nrow(nepal_smote), ncol(nepal_smote)))
```

```{r}
# standard scaling nuermical columns
nepal_smote_scaled <- nepal_smote
nepal_smote_scaled[numeric_cols] <- scale(nepal_smote[numeric_cols])

summary(nepal_smote_scaled[numeric_cols])
```

```{r}
plot_df_2 <- nepal_smote_scaled
if (length(numeric_cols) > 0) {
  plot_df_2 |>
    dplyr::select(all_of(numeric_cols)) |>
    tidyr::pivot_longer(everything(), names_to = "variable", values_to = "value") |>
    ggplot(aes(x = variable, y = value)) +
    geom_boxplot() +
    facet_wrap(~ variable, scales = "free", ncol = 4) +
    theme_minimal(base_size = 12) +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
    labs(title = "Boxplots of Numeric Variables after SMOTE and Standard Scaling", x = NULL, y = "Value")
}
```

```{r}
if (length(numeric_cols) > 0) {
  plot_df_2 |>
    dplyr::select(all_of(numeric_cols)) |>
    tidyr::pivot_longer(everything(), names_to = "variable", values_to = "value") |>
    ggplot(aes(x = value)) +
    geom_histogram(bins = 40) +
    facet_wrap(~ variable, scales = "free", ncol = 4) +
    theme_minimal(base_size = 12) +
    labs(title = "Histograms of Numeric Variables After SMOTE and Standard Scaling", x = "Value", y = "Count")
}
```

# Train/Test Split
```{r}
library(caret)
idx_train <- createDataPartition(nepal_smote_scaled$damage_grade, p = 0.8, list = FALSE)
train <- nepal_smote_scaled[idx_train, ]
test  <- nepal_smote_scaled[-idx_train, ]
```

# GLM with multinomial link
```{r}
library(nnet)

train_glm <- train
test_glm  <- test
train_glm$damage_grade <- factor(train_glm$damage_grade)
test_glm$damage_grade  <- factor(test_glm$damage_grade,
                                 levels = levels(train_glm$damage_grade))

multi_fit <- multinom(
  damage_grade ~ age + area_percentage + height_percentage +
    count_floors_pre_eq + geo_level_1_id + geo_level_2_id + geo_level_3_id +
    land_surface_condition + roof_type + other_floor_type +
    has_superstructure_mud_mortar_stone,
  data  = train_glm,
  trace = TRUE
)
```

```{r}
pred_class_test <- predict(multi_fit, newdata = test_glm)
pred_prob  <- predict(multi_fit, newdata = train_glm, type = "probs")
```

```{r}
# Make sure both are factors
true_y <- factor(test_glm$damage_grade)
pred_y <- factor(pred_class_test, levels = levels(true_y))

# Now compute confusion matrix
library(caret)
cm <- confusionMatrix(pred_y, true_y)
cm
```

# RF
```{r}
library(randomForest)

# If damage_grade is ordered, drop ordering for RF:
train_rf <- train
train_rf$damage_grade <- factor(train_rf$damage_grade)
test_rf  <- test
test_rf$damage_grade  <- factor(test_rf$damage_grade)

rf_mod <- randomForest(
  damage_grade ~ .,
  data = train_rf,
  ntree = 10,
  mtry  = floor(sqrt(ncol(train_rf) - 1)),  # heuristic
  importance = TRUE
)

pred_rf <- predict(rf_mod, newdata = test_rf)

cm_rf <- confusionMatrix(pred_rf, test_rf$damage_grade)
cm_rf 
```

```{r}
library(ranger)

set.seed(521)

train_rf <- train
train_rf$damage_grade <- factor(train_rf$damage_grade)
test_rf  <- test
test_rf$damage_grade  <- factor(test_rf$damage_grade)

p <- ncol(train_rf) - 1  # number of predictors

rf_mod_fast <- ranger(
  formula = damage_grade ~ .,
  data = train_rf,
  num.trees = 600,                     # fewer trees than 500
  mtry = floor(sqrt(p)),              # standard heuristic
  min.node.size = 20,                 # slightly larger leaf size → faster
  importance = "impurity",
  respect.unordered.factors = "order" # handles factors efficiently
)

# Predictions
pred_rf <- predict(rf_mod_fast, data = test_rf)$predictions
pred_rf <- factor(pred_rf, levels = levels(test_rf$damage_grade))

cm_rf_ranger <- caret::confusionMatrix(pred_rf, test_rf$damage_grade)
cm_rf_ranger
```

```{r}
library(ranger)
library(caret)

# --- Variable importance from full ranger model ---

var_imp <- rf_mod_fast$variable.importance
var_imp <- sort(var_imp, decreasing = TRUE)


head(var_imp, 20)

# Choose top 30 predictors

top_k   <- 30
top_vars <- names(var_imp)[1:top_k]

# Build train / test sets restricted to top 30

train_top <- train_rf[, c("damage_grade", top_vars)]
test_top  <- test_rf[,  c("damage_grade", top_vars)]

set.seed(521)
rf_top <- ranger(
formula = damage_grade ~ .,
data    = train_top,
num.trees = 600,
mtry      = floor(sqrt(length(top_vars))),
min.node.size = 20,
importance = "impurity",
respect.unordered.factors = "order"
)

pred_top <- predict(rf_top, data = test_top)$predictions
pred_top <- factor(pred_top, levels = levels(test_top$damage_grade))

cm_rf_top <- caret::confusionMatrix(pred_top, test_top$damage_grade)
cm_rf_top
```


```{r}
library(catboost)

# -------------------------------
# 1. Use SMOTE + scaled dataset
# -------------------------------
dat <- nepal_smote_scaled

# Ensure damage_grade is a proper factor with levels "1","2","3"
dat$damage_grade <- factor(dat$damage_grade, levels = c("1", "2", "3"))

# -------------------------------
# 2. Train/test split (stratified)
# -------------------------------
idx_train <- createDataPartition(dat$damage_grade, p = 0.8, list = FALSE)
train <- dat[idx_train, ]
test  <- dat[-idx_train, ]

# Confirm factor levels are consistent
train$damage_grade <- droplevels(train$damage_grade)
test$damage_grade  <- factor(test$damage_grade,
                             levels = levels(train$damage_grade))

# -------------------------------
# 3. Prepare data for CatBoost
# -------------------------------
# Separate predictors and target
X_train <- subset(train, select = -damage_grade)
X_test  <- subset(test,  select = -damage_grade)

# Numeric class labels for CatBoost: 0,1,2
y_train_num <- as.integer(train$damage_grade) - 1
y_test_num  <- as.integer(test$damage_grade)  - 1

# Create CatBoost pools (all predictors numeric at this point)
train_pool <- catboost.load_pool(
  data  = as.matrix(X_train),
  label = y_train_num
)
test_pool <- catboost.load_pool(
  data  = as.matrix(X_test),
  label = y_test_num
)

# -------------------------------
# 4. Train CatBoost model
# -------------------------------
cat_params <- list(
  loss_function = "MultiClass",
  eval_metric   = "MultiClass",
  iterations    = 3000,
  learning_rate = 0.08,
  depth         = 9,
  l2_leaf_reg   = 5,
  bagging_temperature = 1,
  rsm          = 0.8,
  random_seed  = 521,
  od_type      = "Iter",
  od_wait      = 40
)



cat_model <- catboost.train(
  learn_pool = train_pool,
  test_pool  = NULL,
  params     = cat_params
)

# -------------------------------
# 5. Predictions + confusion matrix
# -------------------------------
# Get class probabilities: n x 3 matrix
pred_prob_cat <- catboost.predict(
  model = cat_model,
  pool  = test_pool,
  prediction_type = "Probability"
)

# Pick class with highest probability (0,1,2)
pred_idx0 <- max.col(pred_prob_cat) - 1

# Map 0,1,2 back to "1","2","3"
dg_levels <- levels(train$damage_grade)  # c("1","2","3")
pred_cat <- factor(dg_levels[pred_idx0 + 1], levels = dg_levels)

# Confusion matrix
cm_cat <- confusionMatrix(pred_cat, test$damage_grade)
cm_cat

# Optional: extract accuracy + balanced accuracy
cat_metrics <- data.frame(
  Accuracy         = cm_cat$overall["Accuracy"],
  BalancedAccuracy = mean(cm_cat$byClass[, "Balanced Accuracy"])
)
cat_metrics
```

```{r}
library(pROC)
true_factor <- test$damage_grade  # levels: "1","2","3"
# Convert true labels into binary indicators
y1 <- ifelse(true_factor == "1", 1, 0)
y2 <- ifelse(true_factor == "2", 1, 0)
y3 <- ifelse(true_factor == "3", 1, 0)

roc1 <- roc(response = y1, predictor = pred_prob_cat[,1])
roc2 <- roc(response = y2, predictor = pred_prob_cat[,2])
roc3 <- roc(response = y3, predictor = pred_prob_cat[,3])

auc1 <- auc(roc1)
auc2 <- auc(roc2)
auc3 <- auc(roc3)

auc1; auc2; auc3
plot(roc1, col = "red",   main = "CatBoost ROC Curves (Multiclass)")
plot(roc2, col = "blue",  add = TRUE)
plot(roc3, col = "green", add = TRUE)

legend("bottomright",
       legend = c(
         paste0("Class 1, AUC=", round(auc1,3)),
         paste0("Class 2, AUC=", round(auc2,3)),
         paste0("Class 3, AUC=", round(auc3,3))
       ),
       col = c("red","blue","green"), lwd = 2)

```

```{r}
library(xgboost)
library(caret)
library(pROC)
library(dplyr)

# -------------------------------
# 1. Use SMOTE + scaled dataset
# -------------------------------
dat <- nepal_smote_scaled

# Make sure damage_grade is a 3-level factor
dat$damage_grade <- factor(dat$damage_grade, levels = c("1", "2", "3"))

# -------------------------------
# 2. Train/test split (stratified)
# -------------------------------
set.seed(521)
idx_train <- createDataPartition(dat$damage_grade, p = 0.8, list = FALSE)
train <- dat[idx_train, ]
test  <- dat[-idx_train, ]

train$damage_grade <- droplevels(train$damage_grade)
test$damage_grade  <- factor(test$damage_grade,
                             levels = levels(train$damage_grade))

# -------------------------------
# 3. Model matrix for XGBoost
# -------------------------------
x_formula <- as.formula("~ . - damage_grade")

X_train <- model.matrix(x_formula, data = train)[, -1]  # drop intercept
X_test  <- model.matrix(x_formula, data = test)[, -1]

y_train_factor <- train$damage_grade
y_test_factor  <- test$damage_grade

# numeric labels 0,1,2 for XGBoost
y_train <- as.numeric(y_train_factor) - 1
y_test  <- as.numeric(factor(y_test_factor,
                             levels = levels(y_train_factor))) - 1

num_class <- nlevels(y_train_factor)

dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest  <- xgb.DMatrix(data = X_test,  label = y_test)
eval_xgb_once <- function(max_depth, eta) {
  params <- list(
    objective        = "multi:softprob",
    eval_metric      = "mlogloss",
    num_class        = num_class,
    max_depth        = max_depth,
    eta              = eta,
    subsample        = 0.8,
    colsample_bytree = 0.8,
    min_child_weight = 1,
    gamma            = 0
  )
  
  watchlist <- list(train = dtrain, eval = dtest)
  
  set.seed(521)
  fit <- xgb.train(
    params = params,
    data   = dtrain,
    nrounds = 1500,
    watchlist = watchlist,
    early_stopping_rounds = 40,
    verbose = 0
  )
  
  # predictions on test
  pred_prob <- predict(fit, dtest)
  pred_mat <- matrix(pred_prob,
                     nrow = length(y_test),
                     ncol = num_class,
                     byrow = TRUE)
  pred_idx <- max.col(pred_mat)
  dg_levels <- levels(y_train_factor)
  pred_factor <- factor(dg_levels[pred_idx], levels = dg_levels)
  
  cm <- confusionMatrix(pred_factor, y_test_factor)
  ba <- mean(cm$byClass[, "Balanced Accuracy"])
  
  tibble(
    max_depth = max_depth,
    eta = eta,
    best_iteration = fit$best_iteration,
    BalancedAccuracy = ba
  )
}
# ---- Step 1: tune depth ----
depth_grid <- c(4, 6, 8)
eta0 <- 0.05

res_depth <- bind_rows(
  lapply(depth_grid, function(d) eval_xgb_once(max_depth = d, eta = eta0))
)

res_depth
best_depth <- res_depth$max_depth[which.max(res_depth$BalancedAccuracy)]
best_depth

# ---- Step 2: tune eta given best_depth ----
eta_grid <- c(0.03, 0.05, 0.08)

res_eta <- bind_rows(
  lapply(eta_grid, function(e) eval_xgb_once(max_depth = best_depth, eta = e))
)

res_eta
best_eta <- res_eta$eta[which.max(res_eta$BalancedAccuracy)]
best_eta
final_params <- list(
  objective        = "multi:softprob",
  eval_metric      = "mlogloss",
  num_class        = num_class,
  max_depth        = best_depth,
  eta              = best_eta,
  subsample        = 0.8,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  gamma            = 0
)

watchlist <- list(train = dtrain, eval = dtest)

set.seed(521)
xgb_final <- xgb.train(
  params = final_params,
  data   = dtrain,
  nrounds = 1500,
  watchlist = watchlist,
  early_stopping_rounds = 40,
  print_every_n = 20
)

xgb_final$best_iteration

# Predict probabilities for test data
pred_prob_final <- predict(xgb_final, dtest)

pred_mat_final <- matrix(pred_prob_final,
                         nrow = length(y_test),
                         ncol = num_class,
                         byrow = TRUE)

pred_idx_final <- max.col(pred_mat_final)
dg_levels <- levels(y_train_factor)
pred_final <- factor(dg_levels[pred_idx_final], levels = dg_levels)

cm_xgb_final <- confusionMatrix(pred_final, y_test_factor)
cm_xgb_final

xgb_metrics_final <- data.frame(
  Accuracy         = cm_xgb_final$overall["Accuracy"],
  BalancedAccuracy = mean(cm_xgb_final$byClass[, "Balanced Accuracy"])
)
xgb_metrics_final
# true labels
true_factor <- y_test_factor  # "1","2","3"

# one-vs-rest binary indicators
y1 <- ifelse(true_factor == "1", 1, 0)
y2 <- ifelse(true_factor == "2", 1, 0)
y3 <- ifelse(true_factor == "3", 1, 0)

# ROC objects
roc1 <- roc(response = y1, predictor = pred_mat_final[, 1])
roc2 <- roc(response = y2, predictor = pred_mat_final[, 2])
roc3 <- roc(response = y3, predictor = pred_mat_final[, 3])

auc1 <- auc(roc1)
auc2 <- auc(roc2)
auc3 <- auc(roc3)

auc1; auc2; auc3

# Macro-average AUC
macro_auc <- mean(c(auc1, auc2, auc3))
macro_auc
plot(roc1, col = "red",   main = "XGBoost ROC Curves (Multiclass)")
plot(roc2, col = "blue",  add = TRUE)
plot(roc3, col = "green", add = TRUE)

legend("bottomright",
       legend = c(
         paste0("Class 1 (damage_grade=1), AUC=", round(auc1, 3)),
         paste0("Class 2 (damage_grade=2), AUC=", round(auc2, 3)),
         paste0("Class 3 (damage_grade=3), AUC=", round(auc3, 3)),
         paste0("Macro AUC=", round(macro_auc, 3))
       ),
       col = c("red", "blue", "green", "black"),
       lwd = 2)

```


```{r}
library(dplyr)

model_compare <- tibble(
  Model = c(
    "Multinomial GLM",
    "Random Forest (randomForest)",
    "Random Forest (ranger, all vars)",
    "Random Forest (ranger, top 30)",
    "CatBoost",
    "XGBoost"
  ),
  Accuracy = c(
    as.numeric(cm$overall["Accuracy"]),
    as.numeric(cm_rf$overall["Accuracy"]),
    as.numeric(cm_rf_ranger$overall["Accuracy"]),
    as.numeric(cm_rf_top$overall["Accuracy"]),
    as.numeric(cm_cat$overall["Accuracy"]),
    as.numeric(cm_xgb_final$overall["Accuracy"])
  ),
  Kappa = c(
    as.numeric(cm$overall["Kappa"]),
    as.numeric(cm_rf$overall["Kappa"]),
    as.numeric(cm_rf_ranger$overall["Kappa"]),
    as.numeric(cm_rf_top$overall["Kappa"]),
    as.numeric(cm_cat$overall["Kappa"]),
    as.numeric(cm_xgb_final$overall["Kappa"])
  ),
  BalancedAccuracy = c(
    mean(cm$byClass[, "Balanced Accuracy"]),
    mean(cm_rf$byClass[, "Balanced Accuracy"]),
    mean(cm_rf_ranger$byClass[, "Balanced Accuracy"]),
    mean(cm_rf_top$byClass[, "Balanced Accuracy"]),
    mean(cm_cat$byClass[, "Balanced Accuracy"]),
    mean(cm_xgb_final$byClass[, "Balanced Accuracy"])
  )
)

model_compare
```


```{r}
library(xgboost)
library(dplyr)
library(readr)

# 1. Read raw train & test 
train_raw <- read_csv("nepal_dat.csv")
test_raw  <- read_csv("nepal_test.csv")

# keep building_id for submission
test_id <- test_raw$building_id

# 2. Make damage_grade same type and combine 
# train has real labels, test has NA in same column
train_raw$damage_grade <- as.character(train_raw$damage_grade)
test_raw$damage_grade  <- NA_character_

all_raw <- bind_rows(train_raw, test_raw)

# 3. Basic preprocessing (numeric vs others) 
numeric_cols <- c(
  "age",
  "area_percentage",
  "height_percentage",
  "count_floors_pre_eq",
  "count_families",
  "geo_level_1_id",
  "geo_level_2_id",
  "geo_level_3_id"
)

all_raw <- all_raw %>%
  mutate(
    across(all_of(numeric_cols), as.numeric),
    damage_grade = factor(damage_grade, levels = c("1", "2", "3"))
  )

# predictor columns: everything except id + response
pred_cols <- setdiff(names(all_raw), c("building_id", "damage_grade"))

# 4. One-hot encode so train/test have identical dummy columns 
x_formula <- as.formula(
  paste("~", paste(pred_cols, collapse = " + "))
)

X_all <- model.matrix(x_formula, data = all_raw)[, -1]  # drop intercept

# split back to train / test rows
n_train <- sum(!is.na(all_raw$damage_grade))
X_train <- X_all[1:n_train, , drop = FALSE]
X_test  <- X_all[(n_train + 1):nrow(X_all), , drop = FALSE]

y_train_factor <- all_raw$damage_grade[1:n_train]
y_train <- as.numeric(y_train_factor) - 1  # 0,1,2 for XGBoost

dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest  <- xgb.DMatrix(data = X_test)

# 5. Fit a reasonable XGBoost model ---------------------------------
params <- list(
  objective        = "multi:softprob",
  eval_metric      = "mlogloss",
  num_class        = 3,
  max_depth        = best_depth,
  eta              = best_eta,
  subsample        = 0.8,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  gamma            = 0
)

set.seed(521)
xgb_sub <- xgb.train(
  params = params,
  data   = dtrain,
  nrounds = 1500,
  watchlist = list(train = dtrain),
  verbose = 0
)

# 6. Predict damage_grade for nepal_test ----------------------------
pred_prob <- predict(xgb_sub, dtest)
pred_mat  <- matrix(pred_prob, ncol = 3, byrow = TRUE)
pred_idx  <- max.col(pred_mat)

pred_labels <- factor(pred_idx, levels = 1:3, labels = c("1", "2", "3"))

submission_xgb <- tibble(
  building_id  = test_id,
  damage_grade = pred_labels
)

head(submission_xgb)

# write file for submission
write_csv(submission_xgb, "nepal_xgb_predictions.csv")


```
